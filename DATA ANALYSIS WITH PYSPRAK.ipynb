{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16d2f73-2b32-4db2-bdc4-373cec1d82ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Rearc Data Analysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6f2c2f-9ffd-45a5-bacb-62227e4399d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cce700e-9cfc-46ad-ba3f-70a98e0b4ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_time_series = spark.read.option(\"header\", True).csv(\"pr.data.0.Current.csv\")\n",
    "df_time_series.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb3d71c7-5be9-4f34-9c01-58448113b77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67ea4fc-144c-4621-bb11-9930517a466e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_population = spark.read.option(\"multiline\", True).json(\"population.json\")\n",
    "df_population.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ee94ac-6daf-425c-bf4d-ba95a73aa620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_population_flat = df_population.selectExpr(\"explode(data) as data\") \\\n",
    "                                   .select(\"data.Year\", \"data.Population\")\n",
    "df_population_flat = df_population_flat.withColumn(\"Year\", df_population_flat[\"Year\"].cast(\"int\")) \\\n",
    "                                       .withColumn(\"Population\", df_population_flat[\"Population\"].cast(\"long\"))\n",
    "df_population_flat.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "075d531a-bd01-4ead-bce7-64fb9b0a5bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtering Years [2013â€“2018] and Compute Mean & Std Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a668756-ec12-4be6-bd0a-dda4fb1919f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev\n",
    "\n",
    "df_filtered_pop = df_population_flat.filter((col(\"Year\") >= 2013) & (col(\"Year\") <= 2018))\n",
    "df_filtered_pop.select(avg(\"Population\").alias(\"Mean_Population\"),\n",
    "                       stddev(\"Population\").alias(\"StdDev_Population\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6e7ca94-fee9-48e2-92aa-b3f55a5bfaaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Clean and Prepare Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b8772b-bebf-4c38-ac59-4d386e6a8d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "df_time_series_clean = df_time_series.select(\n",
    "    trim(col(\"series_id\")).alias(\"series_id\"),\n",
    "    trim(col(\"year\")).cast(\"int\").alias(\"year\"),\n",
    "    trim(col(\"period\")).alias(\"period\"),\n",
    "    trim(col(\"value\")).cast(\"float\").alias(\"value\")\n",
    ").filter(~col(\"period\").like(\"M%\"))  # ignore monthly if needed\n",
    "df_time_series_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bbe9b7e-a643-4206-a6c1-09e7460d8068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Find Best Year per Series ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03df1ff7-0a49-4be0-88ed-739883d38c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "df_yearly_sum = df_time_series_clean.groupBy(\"series_id\", \"year\").agg(_sum(\"value\").alias(\"yearly_value\"))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"series_id\").orderBy(col(\"yearly_value\").desc())\n",
    "\n",
    "df_best_year = df_yearly_sum.withColumn(\"rn\", row_number().over(windowSpec)) \\\n",
    "                            .filter(col(\"rn\") == 1) \\\n",
    "                            .drop(\"rn\")\n",
    "df_best_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4325b26-3a9d-4d1c-90bd-034aaf50da2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_target = df_time_series_clean.filter(\n",
    "    (col(\"series_id\") == \"PRS30006032\") & (col(\"period\") == \"Q01\")\n",
    ")\n",
    "\n",
    "df_combined = df_target.join(\n",
    "    df_population_flat,\n",
    "    df_target.year == df_population_flat.Year,\n",
    "    \"left\"\n",
    ").select(\n",
    "    df_target[\"series_id\"], df_target[\"year\"], df_target[\"period\"],\n",
    "    df_target[\"value\"], df_population_flat[\"Population\"]\n",
    ")\n",
    "\n",
    "df_combined.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DATA ANALYSIS WITH PYSPRAK",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
